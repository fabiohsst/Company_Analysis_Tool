{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Technical Question Explainer Tool\n",
    "\n",
    "This notebook implements a tool that uses OpenAI's API to provide explanations for technical questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55278d7-652e-4007-b6f7-487b92dbbc35",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b26087-804a-4331-a869-c7028b9f8049",
   "metadata": {},
   "source": [
    "This section imports the necessary libraries:\n",
    "- `os`: For environment variable handling\n",
    "- `requests`: For making HTTP requests\n",
    "- `dotenv`: For loading environment variables from a .env file\n",
    "- `IPython.display`: For displaying markdown in Jupyter notebooks\n",
    "- `openai`: The OpenAI API client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3cfaa9-0e68-45c4-8f09-0c541763427b",
   "metadata": {},
   "source": [
    "## Environment Setup and API Key Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# validate API key\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417f53b-460d-4101-8d1c-1f861d6cf1e4",
   "metadata": {},
   "source": [
    "**This section:**\n",
    "1. Loads environment variables from the .env file\n",
    "2. Retrieves the OpenAI API key\n",
    "3. Validates the API key format and presence\n",
    "4. Provides helpful error messages for common issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7ff6a-8576-44fd-9321-1ba425f1c157",
   "metadata": {},
   "source": [
    "## Constants and Client Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b9c5b69-76d4-457d-9ac0-fb9bcb044511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_streaming_response(\n",
    "    prompt: str,\n",
    "    system_prompt: str,\n",
    "    model: str = \"gpt-4o-mini\",\n",
    "    client: openai = None\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generate a streaming response from OpenAI's API and display it as Markdown.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The user's prompt/question to be sent to the API\n",
    "        system_prompt (str): The system prompt that sets the context for the model\n",
    "        model (str, optional): The OpenAI model to use. Defaults to \"gpt-4\"\n",
    "        client (Optional[OpenAI], optional): An OpenAI client instance. If None, creates a new one.\n",
    "    \n",
    "    Returns:\n",
    "        str: The accumulated response from the API\n",
    "        \n",
    "    Example:\n",
    "        >>> system_prompt = \"You're an experienced programmer.\"\n",
    "        >>> user_prompt = \"Explain what this code does: print('Hello, World!')\"\n",
    "        >>> response = get_streaming_response(user_prompt, system_prompt)\n",
    "    \"\"\"\n",
    "    # Initialize OpenAI client if not provided\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "    \n",
    "    # Create messages array\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    # Create streaming response\n",
    "    response_stream = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Accumulate and display response\n",
    "    accumulated_response = \"\"\n",
    "    \n",
    "    for chunk in response_stream:\n",
    "        if chunk.choices:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:\n",
    "                accumulated_response += content\n",
    "    \n",
    "    # Display the markdown response\n",
    "    display(Markdown(accumulated_response))\n",
    "    \n",
    "    return accumulated_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f716a1db-d128-48d5-b96a-c3bc5dcf8e5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CodeExplainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create explainer instance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mCodeExplainer\u001b[49m(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Your code to explain\u001b[39;00m\n\u001b[0;32m      5\u001b[0m code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124myield from \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mbook.get(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m) for book in books if book.get(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)}\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CodeExplainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create explainer instance\n",
    "explainer = CodeExplainer(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Your code to explain\n",
    "code = \"\"\"yield from {book.get(\"author\") for book in books if book.get(\"author\")}\"\"\"\n",
    "\n",
    "# Get explanation\n",
    "explanation = explainer.explain_code(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17ae01-998b-45e3-989b-2d3733517975",
   "metadata": {},
   "source": [
    "**This section:**\n",
    "1. Defines the GPT model to be used\n",
    "2. Initializes the OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9cdbc5-c8ef-4b92-a296-dc041d5274d1",
   "metadata": {},
   "source": [
    "## System Prompt Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "045dc5c7-360d-4a66-8146-e585ea65ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You're experienced programmer, specialized in python code \\\n",
    "I going to give to you a piece of code and you need to explain it to me.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d6b780-de72-4a79-9e57-327eefe43a02",
   "metadata": {},
   "source": [
    "This defines the system prompt that sets the context for the GPT model. The prompt instructs the model to act as an experienced Python programmer who will explain code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaaab43-7587-42b3-845c-5ae5d558740d",
   "metadata": {},
   "source": [
    "## Sample Code for Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"yield from {book.get(\"author\") for book in books if book.get(\"author\")}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9486b-1a5a-4e1c-8c9f-3c0d805884ce",
   "metadata": {},
   "source": [
    "This section defines the sample Python code that will be explained by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aac521-5e4f-45e4-81b1-c63369ef0322",
   "metadata": {},
   "source": [
    "## Message Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d32bdd03-154a-40fe-88cd-df7c3678b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": code}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bcfd2f-ce28-4aba-85a9-a6a7b799cf59",
   "metadata": {},
   "source": [
    "This constructs the message array for the OpenAI API call, including:\n",
    "1. The system message that sets the context\n",
    "2. The user message containing the code to be explained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba507c-f4c0-4e21-be36-63f7c36a26c5",
   "metadata": {},
   "source": [
    "## Basic API Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "731f1d20-e4a3-4f8e-9f4d-344e799705d8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code `yield from {book.get(\"author\") for book in books if book.get(\"author\")}` does several things:\n",
      "\n",
      "1. **Set Comprehension**: The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` creates a set. A set in Python is a collection type that automatically removes duplicate values. \n",
      "   - `book.get(\"author\")` attempts to retrieve the value associated with the key `\"author\"` from each `book` dictionary in the `books` iterable.\n",
      "   - The `if book.get(\"author\")` condition filters out any books that do not have an author (i.e., where `get(\"author\")` returns `None` or an empty string).\n",
      "\n",
      "2. **Yielding Values**: The `yield from` statement is used within a generator function. It allows the generator to yield all values from another iterable (in this case, the set created by the set comprehension) one by one.\n",
      "   - This means that the generator function will return each author in the set, effectively producing them on-the-fly without creating a list or other data structure in memory.\n",
      "\n",
      "### Summary of Functionality:\n",
      "- This code effectively collects all unique authors from a list of `books`, where each `book` is a dictionary that may or may not contain an `\"author\"` key.\n",
      "- It filters out any books that do not have an author and then yields each unique author one at a time.\n",
      "\n",
      "### Example:\n",
      "If `books` is defined as follows:\n",
      "```python\n",
      "books = [\n",
      "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
      "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
      "    {\"title\": \"Book 3\", \"author\": None},\n",
      "    {\"title\": \"Book 4\", \"author\": \"Author A\"},\n",
      "    {\"title\": \"Book 5\", \"author\": \"\"},\n",
      "]\n",
      "```\n",
      "The set comprehension will create a set with the authors `\"Author A\"` and `\"Author B\"` (removing duplicates and ignoring entries without an author). The generator would then yield these authors one by one, allowing you to iterate over them. \n",
      "\n",
      "### Use Cases:\n",
      "This type of pattern is useful in scenarios where you want to efficiently extract and process unique items from a collection, in this case, author names, making it effective for handling data where duplicates may exist.\n"
     ]
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f821c4-59cf-499c-8bf3-b8100c9ee1e8",
   "metadata": {},
   "source": [
    "This demonstrates a basic API call to OpenAI:\n",
    "1. Creates a chat completion request\n",
    "2. Prints the response content from the first choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520d6385-4b80-46ac-b865-6ec64b791891",
   "metadata": {},
   "source": [
    "## Streaming Response Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "200ba610-b4f9-4ed5-b520-c148f2cb643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The line of code you've posted uses a generator function and comprehensions in Python. It employs the `yield from` syntax, which is typically used within generator functions to yield all values from an iterable.\n",
       "\n",
       "Let's break down the statement you have and then provide some context and example usage.\n",
       "\n",
       "### Breakdown of the Code\n",
       "\n",
       "- **Set Comprehension**: `{book.get(\"author\") for book in books if book.get(\"author\")}` is creating a set of authors from a list of books. It iterates through each `book` in the `books` iterable (which is expected to be a list or similar structure), calling `get(\"author\")` on each `book` dictionary to extract the author. Only authors that exist (i.e., are not `None` or falsy) will be included in the set.\n",
       "  \n",
       "- **`yield from`**: This statement allows the function to yield each element from the iterable returned by the set comprehension. The result would be that each author will be yielded one by one.\n",
       "\n",
       "### Example Usage\n",
       "\n",
       "Now let's assume you have the following code to utilize this:\n",
       "\n",
       "```python\n",
       "def get_authors(books):\n",
       "    # Yielding each unique author from the books\n",
       "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "\n",
       "# Example list of books\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\", \"author\": None},\n",
       "    {\"title\": \"Book 4\", \"author\": \"Author A\"},\n",
       "]\n",
       "\n",
       "# Using the generator\n",
       "for author in get_authors(books):\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "### Output\n",
       "This would output:\n",
       "```\n",
       "Author A\n",
       "Author B\n",
       "```\n",
       "\n",
       "### Explanation\n",
       "- The `get_authors` function extracts authors from a list of book dictionaries without returning duplicates (because it uses a set).\n",
       "- When you iterate over `get_authors(books)`, it will yield each unique author found in the books.\n",
       "\n",
       "### Notes\n",
       "1. **Generality**: This approach works well for extracting unique items from a collection.\n",
       "2. **Efficiency**: Using a generator here is memory efficient, especially with large datasets since it does not require the entire list of authors to be stored in memory.\n",
       "3. **Duplicates**: The set ensures that there are no duplicate authors in the output. If you want to retain duplicates while still yielding values, you would not use a set.\n",
       "\n",
       "Feel free to modify the function and the logic inside it based on your specific requirements!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a streaming response generator\n",
    "response_stream = openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": code}\n",
    "    ],\n",
    "    stream=True  # Enable streaming\n",
    ")\n",
    "\n",
    "accumulated_response = \"\"\n",
    "\n",
    "# Read streaming chunks\n",
    "for chunk in response_stream:\n",
    "    if chunk.choices:\n",
    "        content = chunk.choices[0].delta.content\n",
    "        if content:\n",
    "            accumulated_response += content\n",
    "\n",
    "display(Markdown(accumulated_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e286d4b-0cf9-413c-97e5-7dab6a557ceb",
   "metadata": {},
   "source": [
    "This section implements streaming responses:\n",
    "1. Creates a streaming chat completion request\n",
    "2. Accumulates the response chunks\n",
    "3. Displays the final result as markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
